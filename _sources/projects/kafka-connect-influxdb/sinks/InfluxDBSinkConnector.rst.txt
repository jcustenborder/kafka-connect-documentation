=============
InfluxDB Sink
=============

The InfluxDB sink connector is used to write data from a :term:`Kafka topic`_ to an InfluxDB host. When there are more than one record in a batch that have the same measurement, time, and tags, they will be combined to a single point an written to InfluxDB in a batch.



-------------
Configuration
-------------

----------
Connection
----------


^^^^^^^^^^^^
influxdb.url
^^^^^^^^^^^^

The url of the InfluxDB instance to write to.

**Importance:** High

**Type:** String



^^^^^^^^^^^^^^^^^
influxdb.password
^^^^^^^^^^^^^^^^^

The password to connect to InfluxDB with.

**Importance:** High

**Type:** Password

**Default Value:** [hidden]



^^^^^^^^^^^^^^^^^
influxdb.username
^^^^^^^^^^^^^^^^^

The username to connect to InfluxDB with.

**Importance:** High

**Type:** String



^^^^^^^^^^^^^^^^^^^^
influxdb.gzip.enable
^^^^^^^^^^^^^^^^^^^^

Flag to determine if gzip should be enabled.

**Importance:** Low

**Type:** Boolean

**Default Value:** true



^^^^^^^^^^^^^^^^^^
influxdb.log.level
^^^^^^^^^^^^^^^^^^

influxdb.log.level

**Importance:** Low

**Type:** String

**Default Value:** NONE

**Validator:** ValidEnum{enum=LogLevel, allowed=[NONE, BASIC, HEADERS, FULL]}



-----
Write
-----


^^^^^^^^^^^^^^^^^
influxdb.timeunit
^^^^^^^^^^^^^^^^^

The default timeunit for writing data to InfluxDB.

**Importance:** Medium

**Type:** String

**Default Value:** MILLISECONDS

**Validator:** ValidEnum{enum=TimeUnit, allowed=[NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS]}



^^^^^^^^^^^^^^^^^^^^^^^^^^
influxdb.consistency.level
^^^^^^^^^^^^^^^^^^^^^^^^^^

The default consistency level for writing data to InfluxDB.

**Importance:** Low

**Type:** String

**Default Value:** ONE

**Validator:** ValidEnum{enum=ConsistencyLevel, allowed=[ALL, ANY, ONE, QUORUM]}






--------
Examples
--------

^^^^^^^^^^^^^^^^^^^^^^
Property based example
^^^^^^^^^^^^^^^^^^^^^^


This configuration is used typically along with `standalone mode
<http://docs.confluent.io/current/connect/concepts.html#standalone-workers>`_.

.. code-block:: properties
    :name: connector.properties
    :emphasize-lines: 4,5

    name=InfluxDBSinkConnector1
    connector.class=io.confluent.influxdb.InfluxDBSinkConnector
    tasks.max=1
    topics=< Required Configuration >
    influxdb.url=< Required Configuration >




^^^^^^^^^^^^^^^^^^
Rest based example
^^^^^^^^^^^^^^^^^^


This configuration is used typically along with `distributed mode
<http://docs.confluent.io/current/connect/concepts.html#distributed-workers>`_.
Write the following json to `connector.json`, configure all of the required values, and use the command below to
post the configuration to one the distributed connect worker(s). Check here for more information about the
`Kafka Connect REST Interface. <https://docs.confluent.io/current/connect/restapi.html>`_

.. code-block:: json
    :caption: Connect Distributed REST example
    :name: connector.json
    :emphasize-lines: 6,7

    {
      "config" : {
        "name" : "InfluxDBSinkConnector1",
        "connector.class" : "io.confluent.influxdb.InfluxDBSinkConnector",
        "tasks.max" : "1",
        "topics" : "< Required Configuration >",
        "influxdb.url" : "< Required Configuration >"
      }
    }



Use curl to post the configuration to one of the Kafka Connect Workers. Change `http://localhost:8083/` the the endpoint of
one of your Kafka Connect worker(s).

.. code-block:: bash
    :caption: Create a new connector

    curl -s -X POST -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors


.. code-block:: bash
    :caption: Update an existing connector

    curl -s -X PUT -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors/InfluxDBSinkConnector1/config



