=====================
Json Source Connector
=====================

.. image:: SpoolDirJsonSourceConnector.svg


This connector is used to `stream <https://en.wikipedia.org/wiki/JSON_Streaming>` JSON files from a directory while converting the data based on the schema supplied in the configuration.



-------------
Configuration
-------------

-----------
File System
-----------


^^^^^^^^^^
error.path
^^^^^^^^^^

**Importance:** High

**Type:** String

**Validator:** com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@567412db


The directory to place files in which have error(s). This directory must exist and be writable by the user running Kafka Connect.

^^^^^^^^^^^^^
finished.path
^^^^^^^^^^^^^

**Importance:** High

**Type:** String

**Validator:** com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@4addfb95


The directory to place files that have been successfully processed. This directory must exist and be writable by the user running Kafka Connect.

^^^^^^^^^^^^^^^^^^
input.file.pattern
^^^^^^^^^^^^^^^^^^

**Importance:** High

**Type:** String


Regular expression to check input file names against. This expression must match the entire filename. The equivalent of Matcher.matches().

^^^^^^^^^^
input.path
^^^^^^^^^^

**Importance:** High

**Type:** String

**Validator:** com.github.jcustenborder.kafka.connect.utils.config.validators.filesystem.ValidDirectoryWritable@2d3bb293


The directory to read files that will be processed. This directory must exist and be writable by the user running Kafka Connect.

^^^^^^^^^^^^^
halt.on.error
^^^^^^^^^^^^^

**Importance:** High

**Type:** Boolean

**Default Value:** true


Should the task halt when it encounters an error or continue to the next file.

^^^^^^^^^^^^^^^^^^^
file.minimum.age.ms
^^^^^^^^^^^^^^^^^^^

**Importance:** Low

**Type:** Long

**Default Value:** 0

**Validator:** [0,...]


The amount of time in milliseconds after the file was last written to before the file can be processed.

^^^^^^^^^^^^^^^^^^^^^^^^^
processing.file.extension
^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Low

**Type:** String

**Default Value:** .PROCESSING

**Validator:** ValidPattern{pattern=^.*\..+$}


Before a file is processed, it is renamed to indicate that it is currently being processed. This setting is appended to the end of the file.

------
Schema
------


^^^^^^^^^^
key.schema
^^^^^^^^^^

**Importance:** High

**Type:** String


The schema for the key written to Kafka.

^^^^^^^^^^^^
value.schema
^^^^^^^^^^^^

**Importance:** High

**Type:** String


The schema for the value written to Kafka.

-----------------
Schema Generation
-----------------


^^^^^^^^^^^^^^^^^^^^^^^^^
schema.generation.enabled
^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** Boolean

**Default Value:** false


Flag to determine if schemas should be dynamically generated. If set  to true, `key.schema` and `value.schema` can be omitted, but `schema.generation.key.name` and `schema.generation.value.name` must be set.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
schema.generation.key.fields
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** List

**Default Value:** []


The field(s) to use to build a key schema. This is only used during schema generation.

^^^^^^^^^^^^^^^^^^^^^^^^^^
schema.generation.key.name
^^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** String

**Default Value:** com.github.jcustenborder.kafka.connect.model.Key


The name of the generated key schema.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
schema.generation.value.name
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** String

**Default Value:** com.github.jcustenborder.kafka.connect.model.Value


The name of the generated value schema.

----------
Timestamps
----------


^^^^^^^^^^^^^^^
timestamp.field
^^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** String


The field in the value schema that will contain the parsed timestamp for the record. This field cannot be marked as optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html)

^^^^^^^^^^^^^^
timestamp.mode
^^^^^^^^^^^^^^

**Importance:** Medium

**Type:** String

**Default Value:** PROCESS_TIME

**Validator:** ValidEnum{enum=TimestampMode, allowed=[FIELD, FILE_TIME, PROCESS_TIME]}


Determines how the connector will set the timestamp for the [ConnectRecord](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/connector/ConnectRecord.html#timestamp()). If set to `Field` then the timestamp will be read from a field in the value. This field cannot be optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html). Specify the field  in `timestamp.field`. If set to `FILE_TIME` then the last modified time of the file will be used. If set to `PROCESS_TIME` the time the record is read will be used.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
parser.timestamp.date.formats
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Low

**Type:** List

**Default Value:** [yyyy-MM-dd'T'HH:mm:ss, yyyy-MM-dd' 'HH:mm:ss]


The date formats that are expected in the file. This is a list of strings that will be used to parse the date fields in order. The most accurate date format should be the first in the list. Take a look at the Java documentation for more info. https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html

^^^^^^^^^^^^^^^^^^^^^^^^^
parser.timestamp.timezone
^^^^^^^^^^^^^^^^^^^^^^^^^

**Importance:** Low

**Type:** String

**Default Value:** UTC


The timezone that all of the dates will be parsed with.

-------
General
-------


^^^^^
topic
^^^^^

**Importance:** High

**Type:** String


The Kafka topic to write the data to.

^^^^^^^^^^
batch.size
^^^^^^^^^^

**Importance:** Low

**Type:** Int

**Default Value:** 1000


The number of records that should be returned with each batch.

^^^^^^^^^^^^^^^^^^
empty.poll.wait.ms
^^^^^^^^^^^^^^^^^^

**Importance:** Low

**Type:** Long

**Default Value:** 1000

**Validator:** [1,...,9223372036854775807]


The amount of time to wait if a poll returns an empty list of records.




--------
Examples
--------

^^^^
Json
^^^^

This example will read json from the input directory.



Select one of the following configuration methods based on how you have deployed Kafka Connect.
Distributed Mode will the the JSON / REST examples. Standalone mode will use the properties based
example.


**Distributed Mode Json**

.. literalinclude:: SpoolDirJsonSourceConnector.test.example.json
    :language: JSON


**Standalone Mode Properties**

.. literalinclude:: SpoolDirJsonSourceConnector.test.example.properties
    :language: properties




